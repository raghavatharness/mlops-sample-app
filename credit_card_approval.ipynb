{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247af9ba-4924-4b47-a638-6fe4593f1897",
   "metadata": {},
   "source": [
    "\n",
    "# ML model to support approve/deny decision for credit card loan\n",
    "\n",
    "#### sensitive variable: gender (other examples are race, and ethnicity)\n",
    "#### fairness metrics: demographic parity, equal opportunity, and equal accuracy \n",
    "\n",
    "References:\n",
    "- https://www.kaggle.com/code/alexisbcook/exercise-ai-fairness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a0168",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52eea240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import logging\n",
    "import joblib, datetime, os, time, yaml\n",
    "from os import environ as env\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import base64\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a859789",
   "metadata": {},
   "source": [
    "# configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b66cd769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_file_path = 'config.yml'\n",
    "model_name = 'Decision Tree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25637c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee18deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)-15s %(message)s\")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3c691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "with open(config_file_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# set model related parameters\n",
    "data_file_path = config['data']['load_file_path']\n",
    "selected_model_file_path = config['model']['save_file_path']\n",
    "selected_model_metrics_file_path = config['model']['save_metrics_file_path']\n",
    "\n",
    "# set MLFlow related parameters\n",
    "experiment_name = config[\"mlflow\"][\"experiment_name\"]\n",
    "run_name = config[\"mlflow\"][\"run_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7eac74",
   "metadata": {},
   "source": [
    "# experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84bfce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_Children</th>\n",
       "      <th>Group</th>\n",
       "      <th>Income</th>\n",
       "      <th>Own_Car</th>\n",
       "      <th>Own_Housing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288363</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40690</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64982</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>75469</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227641</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>70497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137672</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56666</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Num_Children  Group  Income  Own_Car  Own_Housing\n",
       "288363             1      1   40690        0            1\n",
       "64982              2      0   75469        1            0\n",
       "227641             1      1   70497        1            1\n",
       "137672             1      1   61000        0            0\n",
       "12758              1      1   56666        1            1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "data = pd.read_csv(data_file_path)\n",
    "X = data.drop([\"Target\"], axis=1)\n",
    "y = data[\"Target\"]\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Data successfully loaded!\\n\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15aca5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Confusion matrix with saving plot functionality\n",
    "def get_confusion_matrix(estimator, X, y_true, y_pred, display_labels=[\"Deny\", \"Approve\"],\n",
    "                         include_values=True, xticks_rotation='horizontal', values_format='',\n",
    "                         normalize=None, cmap=plt.cm.Blues, file_prefix=''):\n",
    "    matrix = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=display_labels)\n",
    "    disp.plot(include_values=include_values, cmap=cmap, xticks_rotation=xticks_rotation, values_format=values_format)\n",
    "    plt.savefig(f\"{file_prefix}confusion_matrix.jpg\",  bbox_inches='tight')  # Save plot as JPEG file\n",
    "    return matrix, disp\n",
    "\n",
    "# Fairness stats with saving plot functionality\n",
    "def get_fairness_stats(X, y, model, group_one, preds):\n",
    "    fairness_stats = {}\n",
    "        \n",
    "    y_zero, preds_zero, X_zero = y[group_one==False], preds[group_one==False], X[group_one==False]\n",
    "    y_one, preds_one, X_one = y[group_one], preds[group_one], X[group_one]\n",
    "    \n",
    "    # Overall confusion matrix\n",
    "    cm, disp = get_confusion_matrix(model, X, y, preds, file_prefix='overall_')\n",
    "    disp.ax_.set_title(\"Overall\")\n",
    "    \n",
    "    # Group 0 confusion matrix\n",
    "    cm_zero, disp_zero = get_confusion_matrix(model, X_zero, y_zero, preds_zero, file_prefix='group_0_')\n",
    "    disp_zero.ax_.set_title(\"Group 0\")\n",
    "    \n",
    "    # Group 1 confusion matrix\n",
    "    cm_one, disp_one = get_confusion_matrix(model, X_one, y_one, preds_one, file_prefix='group_1_')\n",
    "    disp_one.ax_.set_title(\"Group 1\")\n",
    "    \n",
    "    fairness_stats['demographic_parity'] = {'total_number_of_approvals': preds.sum(), \n",
    "                                            'group_0_%': round(preds_zero.sum()/sum(preds)*100, 2),\n",
    "                                            'group_1_%': round(preds_one.sum()/sum(preds)*100, 2)}         \n",
    "    fairness_stats['equal_opportunity'] = {'true_positive_rate': round(cm[1,1] / cm[1].sum()*100, 2),\n",
    "                                           'group_0_%': round(cm_zero[1,1] / cm_zero[1].sum()*100, 2),\n",
    "                                           'group_1_%': round(cm_one[1,1] / cm_one[1].sum()*100, 2)}\n",
    "    fairness_stats['equal_accuracy'] = {'overall_accuracy': round((preds==y).sum()/len(y)*100, 2),\n",
    "                                        'group_0_%': round((preds_zero==y_zero).sum()/len(y_zero)*100, 2),\n",
    "                                        'group_1_%': round((preds_one==y_one).sum()/len(y_one)*100, 2)}\n",
    "    fairness_stats['confusion_matrix'] = {'overall_confusion_matrix': cm.tolist(), \n",
    "                                          'group_0': cm_zero.tolist(),\n",
    "                                          'group_1': cm_one.tolist()}\n",
    "    return fairness_stats\n",
    "\n",
    "# General metrics function\n",
    "def get_metrics(y_true, y_pred):\n",
    "    metrics = {}\n",
    "    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred).tolist()\n",
    "    metrics['precision'] = precision_score(y_true, y_pred)\n",
    "    metrics['recall'] = recall_score(y_true, y_pred)\n",
    "    metrics['f1'] = f1_score(y_true, y_pred)\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    return metrics\n",
    "\n",
    "# Feature importance with saving plot functionality\n",
    "def get_feature_importances(X, y):\n",
    "    feature_names = X.columns\n",
    "    forest = RandomForestClassifier(n_estimators= 120,min_samples_split = 3,  random_state=0)\n",
    "    forest.fit(X, y)\n",
    "\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "    forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "    ax.set_title(\"Feature importances using Mean Decrease in Impurity\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    plt.savefig(\"feature_importances.jpg\",  bbox_inches='tight')  # Save plot as JPEG file\n",
    "    return forest_importances.to_json()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3855992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "\n",
    "# train\n",
    "model_baseline = DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "model_baseline.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_train_pred = model_baseline.predict(X_train)\n",
    "y_test_pred = model_baseline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "train_metrics = get_metrics(y_train, y_train_pred)\n",
    "test_metrics = get_metrics(y_test, y_test_pred)\n",
    "\n",
    "fairness_metrics = get_fairness_stats(X_test, y_test, model_baseline, X_test[\"Group\"]==1, y_test_pred)\n",
    "feature_importances = get_feature_importances(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58007d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualize_model(model, feature_names, class_names=[\"Deny\", \"Approve\"], impurity=False, filename=\"model_visualization.jpg\"):\n",
    "    plt.figure(figsize=(20, 6))  # Moved inside the function to ensure a new figure is created for each call\n",
    "    plot_list = plot_tree(model, feature_names=feature_names, class_names=class_names, impurity=impurity)\n",
    "    [process_plot_item(item) for item in plot_list]\n",
    "    plt.savefig(filename,  bbox_inches='tight')  # Save the plot as a JPEG file\n",
    "\n",
    "def process_plot_item(item):\n",
    "    split_string = item.get_text().split(\"\\n\")\n",
    "    if split_string[0].startswith(\"samples\"):\n",
    "        item.set_text(\"\\n\".join(split_string[1:]))  # Show only the class names\n",
    "    else:\n",
    "        item.set_text(split_string[0])\n",
    "\n",
    "# Example of how to call visualize_model\n",
    "# model_baseline is your trained model and X_train.columns are the feature names\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plot_list = visualize_model(model_baseline, feature_names=X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45552f53-77e0-4538-8868-976c2cbbfd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d40678-29c1-478e-9fb5-7d617e597cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfbce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender unaware model\n",
    "\n",
    "# remove gender\n",
    "X_train_unaware = X_train.drop([\"Group\"],axis=1)\n",
    "X_test_unaware = X_test.drop([\"Group\"],axis=1)\n",
    "\n",
    "# train\n",
    "model_unaware = DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "model_unaware.fit(X_train_unaware, y_train)\n",
    "\n",
    "# predict\n",
    "y_train_unaware_pred = model_unaware.predict(X_train_unaware)\n",
    "y_test_unaware_pred = model_unaware.predict(X_test_unaware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912e7e9-ac6c-4103-abba-e202eb1d697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction example\n",
    "# model_unaware.predict(X_train_unaware.iloc[0:1].values).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "train_metrics = get_metrics(y_train, y_train_unaware_pred)\n",
    "test_metrics = get_metrics(y_test, y_test_unaware_pred)\n",
    "\n",
    "fairness_metrics = get_fairness_stats(X_test, y_test, model_unaware, X_test[\"Group\"]==1, y_test_pred)\n",
    "\n",
    "feature_importances = get_feature_importances(X_train_unaware, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d883f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plot_list = visualize_model(model_unaware, feature_names=X_train_unaware.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold finetuning to further mitigate bias\n",
    "\n",
    "zero_threshold = 0.11\n",
    "one_threshold = 0.99\n",
    "\n",
    "# predict\n",
    "test_probs = model_unaware.predict_proba(X_test_unaware)[:,1]\n",
    "preds_approval = (((test_probs>zero_threshold)*1)*[X_test[\"Group\"]==0] + ((test_probs>one_threshold)*1)*[X_test[\"Group\"]==1])[0]\n",
    "get_fairness_stats(X_test, y_test, model_unaware, X_test[\"Group\"]==1, preds_approval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e64a5",
   "metadata": {},
   "source": [
    "# save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019a56c-5441-4a6d-9531-85f1acf1a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33917637-26b7-4959-9e7d-05747523fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save selected model and metrics\n",
    "selected_model = model_unaware\n",
    "selected_model_metrics = {'train': train_metrics,\n",
    "                          'test': test_metrics,\n",
    "                          'fairness': fairness_metrics,\n",
    "                          'feature_importances': feature_importances}\n",
    "\n",
    "joblib.dump(selected_model, selected_model_file_path)\n",
    "joblib.dump(selected_model_metrics, selected_model_metrics_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf8570-83d2-4ade-8ae2-83b7226ff0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dictionary to a file in JSON format\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Custom encoder for numpy data types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "with open(selected_model_metrics_file_path.replace('joblib','json'), 'w') as file:\n",
    "    json.dump(selected_model_metrics, file,  cls=NumpyEncoder, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b09eb6-351d-452e-a3bd-332b7ae591cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e4de094",
   "metadata": {},
   "source": [
    "# track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf34cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track experiment \n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "mlflow.log_params(\n",
    "    {\n",
    "        \"model_type\": model_name,\n",
    "        \"n_features\": X.shape[1],\n",
    "        \"feature_importances\": feature_importances\n",
    "    }\n",
    ")\n",
    "\n",
    "mlflow.log_metric('train_f1', selected_model_metrics['train']['f1'])\n",
    "mlflow.log_metric('test_f1', selected_model_metrics['test']['f1'])\n",
    "   \n",
    "mlflow.log_artifact(f\"{selected_model_file_path}\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de050a",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280dcd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_CHECK_OUTPUT\n",
    "selected_model_metrics = joblib.load(selected_model_metrics_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_CHECK_OUTPUT\n",
    "# model accuracy\n",
    "threshold_accuracy = 0.9\n",
    "accuracy = selected_model_metrics['test']['accuracy']\n",
    "assert accuracy > threshold_accuracy, 'accuracy {} is below {}'.format(accuracy, threshold_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345bd6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_CHECK_OUTPUT\n",
    "# fairness\n",
    "equal_opportunity_fairness_percent_threshold = 21\n",
    "equal_opportunity_group_0_percent = selected_model_metrics['fairness']['equal_opportunity']['group_0_%']\n",
    "equal_opportunity_group_1_percent = selected_model_metrics['fairness']['equal_opportunity']['group_1_%']\n",
    "equal_opportunity_fairness_percent = abs(equal_opportunity_group_0_percent - equal_opportunity_group_1_percent)\n",
    "assert equal_opportunity_fairness_percent < equal_opportunity_fairness_percent_threshold, 'difference exceeds {}%'.format(equal_opportunity_fairness_percent_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variables to propagate down the pipeline\n",
    "#os.environ('ACCURACY') = str(selected_model_metrics['test']['accuracy'])\n",
    "#os.environ('EQUAL_OPPORTUNITY_FAIRNESS_PERCENT') = str(equal_opportunity_fairness_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260de52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "directory = 'output/'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Write values to a file\n",
    "shared_file_path = 'output/shared_env_variables.txt'\n",
    "with open(shared_file_path, 'w') as file:\n",
    "    file.write(f'ACCURACY={accuracy}\\nEQUAL_OPPORTUNITY_FAIRNESS_PERCENT={equal_opportunity_fairness_percent}\\n')\n",
    "\n",
    "print(f'Values written to output/shared_env_variables.txt: ACCURACY={accuracy}, EQUAL_OPPORTUNITY_FAIRNESS_PERCENT={equal_opportunity_fairness_percent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd541394-4dbf-4211-a754-863b21b7b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = selected_model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5cb0a-b84e-4f5e-b3bd-78081e307cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics Visualization\n",
    "performance_metrics = ['precision', 'recall', 'f1', 'accuracy']\n",
    "train_performance = [data['train'][metric] for metric in performance_metrics]\n",
    "test_performance = [data['test'][metric] for metric in performance_metrics]\n",
    "\n",
    "ind = np.arange(len(performance_metrics))  # the x locations for the groups\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind - width/2, train_performance, width, label='Train')\n",
    "rects2 = ax.bar(ind + width/2, test_performance, width, label='Test')\n",
    "\n",
    "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Performance Metrics by Train and Test')\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(performance_metrics)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"Performance Metrics by Train and Test.jpg\",  bbox_inches='tight')  # Save plot as JPEG file\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Fairness Metrics Visualization - Demographic Parity and Equal Opportunity\n",
    "fairness_metrics = ['demographic_parity', 'equal_opportunity', 'equal_accuracy']\n",
    "groups = ['group_0_%', 'group_1_%']\n",
    "fairness_values = {metric: [data['fairness'][metric][group] for group in groups] for metric in fairness_metrics}\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for ax, (metric, values) in zip(axs, fairness_values.items()):\n",
    "    ax.bar(groups, values)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_ylabel('Percentage' if metric != 'equal_accuracy' else 'Accuracy %')\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Fairness Metrics Visualization - Demographic Parity and Equal Opportunity.jpg\",  bbox_inches='tight')  # Save plot as JPEG file\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8971f69e-072d-49fd-bfe3-a2b6cbda3052",
   "metadata": {},
   "source": [
    "## Save in one markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e31ccc-bd18-4d88-b503-f3d3f302bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27ef1d-54c9-44a1-9ed9-d101cb5806f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_filenames = [\n",
    "    'Performance Metrics by Train and Test.jpg',\n",
    "    'feature_importances.jpg',\n",
    "    'model_visualization.jpg',\n",
    "    'Fairness Metrics Visualization - Demographic Parity and Equal Opportunity.jpg',\n",
    "    'overall_confusion_matrix.jpg'\n",
    "]\n",
    "\n",
    "descriptions = [\n",
    "    \"Performance Metrics by Train and Test\",\n",
    "    \"Feature Importances\",\n",
    "    \"Model Visualization\",\n",
    "    \"Fairness Metrics Visualization - Demographic Parity and Equal Opportunity\",\n",
    "    \"Overall Confusion Matrix\"\n",
    "]\n",
    "\n",
    "output_filename = 'output/model_metrics.html'  # Adjust the path as necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9b758-c37b-4bc3-8d12-5b1e783c18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def images_to_base64_html(image_filenames, descriptions, output_filename='combined_images.html'):\n",
    "    \"\"\"\n",
    "    Embeds a list of images in JPEG format into an HTML file using Base64 encoding, with descriptions as headings.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_filenames: List[str]. A list of paths to the image files.\n",
    "    - descriptions: List[str]. A list of descriptions for the images.\n",
    "    - output_filename: str. The name of the output HTML file.\n",
    "    \"\"\"\n",
    "    # Ensure the number of descriptions matches the number of images\n",
    "    if len(image_filenames) != len(descriptions):\n",
    "        print(\"Error: The number of descriptions must match the number of images.\")\n",
    "        return\n",
    "\n",
    "    # Extract the directory name from the output_filename\n",
    "    output_dir = os.path.dirname(output_filename)\n",
    "    \n",
    "    # Ensure the output directory exists, if a directory path is provided\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Start writing to the HTML file\n",
    "    with open(output_filename, 'w') as html_file:\n",
    "        html_file.write(\"<html>\\n<head>\\n<title>All Plots</title>\\n</head>\\n<body>\\n\")\n",
    "        for image_filename, description in zip(image_filenames, descriptions):\n",
    "            # Ensure the file exists and is a JPEG image\n",
    "            if os.path.exists(image_filename) and (image_filename.lower().endswith('.jpeg') or image_filename.lower().endswith('.jpg')):\n",
    "                # Encode the image to Base64\n",
    "                with open(image_filename, 'rb') as image_file:\n",
    "                    encoded_string = base64.b64encode(image_file.read()).decode()\n",
    "                    # Write the description as a heading and the image in Base64 format to the HTML file\n",
    "                    html_file.write(f\"<h1>{description}</h1>\\n<img src='data:image/jpeg;base64,{encoded_string}' />\\n\")\n",
    "            else:\n",
    "                print(f\"File does not exist or is not a JPEG image: {image_filename}\")\n",
    "        html_file.write(\"</body>\\n</html>\")\n",
    "\n",
    "\n",
    "\n",
    "# Make sure the path exists or is adjusted according to your environment before running the function\n",
    "images_to_base64_html(image_filenames, descriptions, output_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86de35ed-b2fc-4e77-a99d-d3896359889b",
   "metadata": {},
   "source": [
    "## mlflow tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84fb80-3d66-467a-bdb6-82105b3386d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI')\n",
    "# read from higher level config > \"http://ec2-3-144-250-80.us-east-2.compute.amazonaws.com:5000\"\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "mlflow.set_tag('HARNESS_PIPELINE_ID', os.getenv('HARNESS_PIPELINE_ID'))\n",
    "\n",
    "mlflow.log_params({\n",
    "    \"model_type\": model_name,\n",
    "    \"n_features\": X.shape[1],\n",
    "    \"feature_importances\": feature_importances\n",
    "})\n",
    "\n",
    "performance_metrics = ['precision', 'recall', 'f1', 'accuracy']\n",
    "for metric in performance_metrics:\n",
    "    mlflow.log_metric('train_'+metric, selected_model_metrics['train'][metric])\n",
    "    mlflow.log_metric('test_'+metric, selected_model_metrics['test'][metric])\n",
    "                        \n",
    "mlflow.log_artifact(f\"{selected_model_file_path}\")\n",
    "for ifile in image_filenames:\n",
    "    mlflow.log_artifact(ifile)\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. prepare data: (raw_data -> processed_data)\n",
    "    - train/test split\n",
    "    - cleaning\n",
    "    - data profiling\n",
    "    - exploratory data analysis (generate hypothesis for features)\n",
    "    \n",
    "2. train model: (processed_data -> selected_model_artifacts)\n",
    "    - fit on train data\n",
    "    - evaluate on train data\n",
    "    - evaluate on test data\n",
    "    - track experiments with MLFlow for several models\n",
    "    - select best performing model\n",
    "    - save model\n",
    "    \n",
    "3. unit and integration test: (selected_model_artifacts -> test_report)\n",
    "    - code test cases\n",
    "    - model metrics tests (overfit check, accuracy threshold, fairness threshold etc)\n",
    "    - policy checks\n",
    "    \n",
    "4. deploy: (selected_model_artifacts -> rest_endpoint)\n",
    "    - deploy to AWS Lambda \n",
    "    \n",
    "5. inference: (features -> predictions)\n",
    "    - batch/streaming\n",
    "    \n",
    "6. monitor: (predictions -> alerts)\n",
    "    - data drift\n",
    "    - model performance drift\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
